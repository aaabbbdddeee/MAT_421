{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO81awuj02d2in7g+JYZPwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaabbbdddeee/MAT_421/blob/main/Assignment13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Probability Distribution*"
      ],
      "metadata": {
        "id": "eDert6b61PhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional probability is the probability of an event occurring based on the occurrence of a previous event. It is expressed as the ratio of unconditional probabilities. The multiplication rule states that the probability of the intersection of two events is equal to the product of the probability of one event and the conditional probability of the other event given the first event. Events are considered independent if the occurrence or nonoccurrence of one event has no bearing on the chance that the other will occur. A random variable is a measurable function that maps from the sample space to the real numbers. There are two types of random variables: discrete and continuous. A probability mass function gives the probability that a discrete random variable is exactly equal to some value. The probability distribution of a discrete random variable is defined for every number x, and the cumulative distribution function is defined for every number x as the sum of the probability mass functions up to and including x. The binomial random variable is defined as the number of successes among n trials in independent Bernoulli experiments, and its probability mass function and cumulative distribution function have specific forms."
      ],
      "metadata": {
        "id": "Y93VqWtf1utY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Independent Variables and Random Samples*"
      ],
      "metadata": {
        "id": "sDizbBGJ1viD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In probability theory, joint probability is the probability of two or more events happening together. A joint probability distribution shows the probability distribution for two (or more) random variables. For two discrete random variables X and Y, the joint probability mass function is defined by P(X = x and Y = y), and for continuous random variables X and Y, the joint probability density function is defined by f(x,y). The marginal distribution of a subset of random variables is the probability distribution of the variables contained in the subset without reference to the values of the other variable. Two random variables X and Y are said to be independent if the probability of their joint occurrence is equal to the product of their individual probabilities."
      ],
      "metadata": {
        "id": "ISE7lo3B2BYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Maximum Likelihood Estimation*"
      ],
      "metadata": {
        "id": "GbeeIRTt2BxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In statistics, Maximum Likelihood Estimation (MLE) is a technique used to estimate the parameters of a probability distribution. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. This method has become a dominant means of statistical inference due to its flexibility and intuitiveness.\n",
        "\n",
        "To find the MLE, we first have to define the likelihood function as a joint pmf or pdf of the random variables in a sample. The MLE estimates the values of the unknown parameters that maximize the likelihood function. For instance, when we have a random sample from a normal distribution, we can use the likelihood function to find the maximizing values of the mean and variance.\n",
        "\n",
        "Linear regression is another statistical method that can be viewed from a probabilistic point of view using MLE. Given input data points, we seek an affine function to fit the data. The common approach involves finding coefficients that minimize the criterion, which is the sum of the squares of the differences between the observed and predicted values. From a probabilistic point of view, we can assume that the data points are drawn independently and identically from the normal distribution, and then we can use the likelihood function to estimate the coefficients that maximize the probability of the data points being drawn."
      ],
      "metadata": {
        "id": "YnDgCRmQ2FIP"
      }
    }
  ]
}